{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\r\n",
    "from nltk.tokenize import sent_tokenize\r\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\r\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\generation\\utils.py:1357: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Who wrote The Hound of the Baskervilles?\n",
      "A. Who wrote The Hound of the Baskervilles?\n",
      "B. Who wrote The Hound of the Baskervilles?\n",
      "C. Who wrote The Hound of the Baskervilles?\n",
      "D. Who wrote The Hound of the Baskervilles?\n",
      "Answer key: A\n",
      "\n",
      "Question 2: Who is A. Conan Doyle?\n",
      "A. What is the name of the person who is the most famous person?\n",
      "B. What is the name of the person who is the most famous person?\n",
      "C. What is the name of the person who is the most truly a person?\n",
      "D. What is the name of the person who is the most famous person?\n",
      "Answer key: C\n",
      "\n",
      "Question 3: What chapter is the Curse of the Baskervilles Chapter 3 The Problem Chapter 4\n",
      "A. What chapter is the Curse of the Baskervilles?\n",
      "B. What chapter is the Curse of the Baskervilles?\n",
      "C. What chapter is the Curse of the Baskervilles?\n",
      "D. What chapter is the Curse of the Baskervilles?\n",
      "Answer key: A\n",
      "\n",
      "Question 4: What was the name of the stick that was left behind by the visitor?\n",
      "A. What was the name of the stick that was left behind by the visitor?\n",
      "B. What was the name of the stick that was left behind by the visitor?\n",
      "C. What was the name of the stick that was left behind by the visitor?\n",
      "D. What was the name of the stick that was left behind by the visitor?\n",
      "Answer key: A\n",
      "\n",
      "Question 5: What did Holmes say about Holmes?\n",
      "A. What did Holmes do?\n",
      "B. What did Holmes do?\n",
      "C. What did Holmes do?\n",
      "D. What did Holmes do?\n",
      "Answer key: A\n",
      "\n",
      "Question 6: What is the name of the person who made the visitor's stick?\n",
      "A. What is the name of the person who made the visitor's stick?\n",
      "B. What is the name of the person who made the visitor's stick?\n",
      "C. What is the name of the person who made the visitor's stick?\n",
      "D. What is the name of the person who made the visitor's stick?\n",
      "Answer key: A\n",
      "\n",
      "Question 7: What is the local hunt to whose members he has possibly given some surgical assistance?\n",
      "A. What is the name of the hunt that Watson has given to whose members?\n",
      "B. What is the name of the hunt that Watson has given to whose members?\n",
      "C. What is the name of the hunt that Watson has given to whose members?\n",
      "D. What is the name of the hunt that Watson has given to whose members?\n",
      "Answer key: A\n",
      "\n",
      "Question 8: What did Watson say he was referring to as a child?\n",
      "A. What did Watson say he was referring to as a child?\n",
      "B. What did Watson say he was referring to as a child?\n",
      "C. What did Watson say he was referring to as a child?\n",
      "D. What did Watson say he was referring to as a child?\n",
      "Answer key: A\n",
      "\n",
      "Question 9: What did you say that you stimulated me?\n",
      "A. What did you say that you stimulated me?\n",
      "B. What did you say that you stimulated me?\n",
      "C. What did you say that you stimulated me?\n",
      "D. What did you say that you stimulated me?\n",
      "Answer key: A\n",
      "\n",
      "Question 10: What would it be most probable that a presentation would be made?\n",
      "A. What would it be most probable that a presentation would be made?\n",
      "B. What would it be most probable that a presentation would be made?\n",
      "C. What would it be most probable that a presentation would be made?\n",
      "D. What would it be most probable that a presentation would be made?\n",
      "Answer key: A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\r\n",
    "    document = fitz.open(pdf_path)\r\n",
    "    text = \"\"\r\n",
    "    for page_num in range(len(document)):\r\n",
    "        page = document.load_page(page_num)\r\n",
    "        text += page.get_text()\r\n",
    "    return text\r\n",
    "\r\n",
    "def process_text(text):\r\n",
    "    sentences = sent_tokenize(text)\r\n",
    "    return sentences\r\n",
    "\r\n",
    "def generate_question_and_answer(text, model, tokenizer):\r\n",
    "    input_text_q = \"generate question: \" + text\r\n",
    "    input_ids_q = tokenizer.encode(input_text_q, return_tensors=\"pt\")\r\n",
    "    output_q = model.generate(input_ids_q)\r\n",
    "    question = tokenizer.decode(output_q[0], skip_special_tokens=True)\r\n",
    "\r\n",
    "    input_text_a = \"generate answer: \" + text\r\n",
    "    input_ids_a = tokenizer.encode(input_text_a, return_tensors=\"pt\")\r\n",
    "    output_a = model.generate(input_ids_a)\r\n",
    "    correct_answer = tokenizer.decode(output_a[0], skip_special_tokens=True)\r\n",
    "    \r\n",
    "    incorrect_answers = []\r\n",
    "    for _ in range(3):\r\n",
    "        input_text_incorrect = \"generate incorrect answer: \" + text\r\n",
    "        input_ids_incorrect = tokenizer.encode(input_text_incorrect, return_tensors=\"pt\")\r\n",
    "        output_incorrect = model.generate(input_ids_incorrect)\r\n",
    "        incorrect_answer = tokenizer.decode(output_incorrect[0], skip_special_tokens=True)\r\n",
    "        incorrect_answers.append(incorrect_answer)\r\n",
    "    \r\n",
    "    all_answers = incorrect_answers + [correct_answer]\r\n",
    "    random.shuffle(all_answers)\r\n",
    "    \r\n",
    "    all_answers = [answer.split(\"<sep>\")[0].strip() for answer in all_answers]\r\n",
    "    question = question.split(\"<sep>\")[0].strip()\r\n",
    "    correct_answer = correct_answer.split(\"<sep>\")[0].strip()\r\n",
    "    \r\n",
    "    return question, correct_answer, all_answers\r\n",
    "\r\n",
    "def main(pdf_path):\r\n",
    "    text = extract_text_from_pdf(pdf_path)\r\n",
    "    sentences = process_text(text)\r\n",
    "    sentences = sentences[:10]\r\n",
    "    \r\n",
    "    model_name = \"valhalla/t5-small-e2e-qg\"\r\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\r\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\r\n",
    "    \r\n",
    "    qa_pairs = [generate_question_and_answer(sentence, model, tokenizer) for sentence in sentences]\r\n",
    "    \r\n",
    "    return qa_pairs\r\n",
    "\r\n",
    "pdf_path = \"./The-Hound-of-the-Baskervilles.pdf\"\r\n",
    "qa_pairs = main(pdf_path)\r\n",
    "\r\n",
    "for i, (question, correct_answer, all_answers) in enumerate(qa_pairs):\r\n",
    "    print(f\"Question {i+1}: {question}\")\r\n",
    "    for j, answer in enumerate(all_answers):\r\n",
    "        print(f\"{chr(65 + j)}. {answer}\")\r\n",
    "    correct_index = all_answers.index(correct_answer)\r\n",
    "    print(f\"Answer key: {chr(65 + correct_index)}\")\r\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13427881f06f13311079f5221e5dd632fdf9146891f6da22d47a93dcb9272d3a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}